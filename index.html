<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Simin Li</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
<!--     <link rel="icon" type="image/jpg" href="https://htqin.github.io/buaa_icon.jpg"> -->
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Simin Li (李思民)</name></p>
                <p align="justify">I am a Ph.D. student (2021.09-) at the State Key Laboratory of Software Development Environment (SKLSDE) and <a href="https://scse.buaa.edu.cn/">School of Computer Science and Engineering (SCSE)</a> <a href="https://scse.buaa.edu.cn/"></a>, <a href="https://www.buaa.edu.cn/">Beihang University</a>, Beijing, China, supervised by Prof. <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>. I also receive supervision from Prof. <a href="https://www.yangyaodong.com/">Yaodong Yang</a> at Peking University (2023.03-). I am a visiting scholar (2024.06 - 2025.05) at Nanyang Technological University (NTU), under the supervision of Prof. <a href="https://personal.ntu.edu.sg/boan/">Bo An</a>. Before that, I obtained my BSc degree in 2020 from <a href="https://www.buaa.edu.cn/">Beihang University</a> (<i>Summa Cum Laude</i>).
                </p>
                <font color="red">new</font> I expect to gratuate at November 2025. I will be joining the Chinese University of Hong Kong as a Honorary Research Assistant from 2025.07-2025.09. After PhD graduation, I will be joining the Chinese University of Hong Kong as a postdoc, supervised by Prof. <a href="https://www.cse.cuhk.edu.hk/~qdou/">Qi Dou</a>.
                </p>
                </p>
                <font color="red">[Prospective students]</font> Our group has positions for PhD students, Master students, and visiting students. If you are interested, please send me an email with your CV and publications (if any).
                </p>
                </p>
                <font color="red">[Cooperations]</font> I am open to cooperation with companies. I am particularly interested in research on single/multi-agent control and their robustness. I am also interested in AI alignment and deception. Please send me an email for contact.
                </p>
                    <br><br>

                    <strong>Email:</strong> lisiminsimon@buaa.edu.cn
                <br>

                </p><p align="center">
                    <a href="https://scholar.google.com/citations?user=uo8l5ckAAAAJ&hl">Google Scholar</a> /
                    <a href="./CV-SiminLi.pdf">CV</a>
<!--                     <a href="https://www.zhihu.com/column/c_1343181023409074176">知乎专栏</a> -->
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./photo1.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">
		    I work on <strong>Trustworthy AI</strong> for <strong>multi-agent reinforcement learning (MARL)</strong> during my PhD. My research goal is to make reinforcement learning safe and robust, including practical adversarial attack for RL/MARL, robustness evaluation of MARL and adversarial defense.
            </p>
            <font color="red">new</font> I am currently working on <strong>Vision-Language-Action fundation models</strong> and <strong>AI Alignment</strong>. New works coming soon.
            </p>
            Now my research mainly includes:
                <ul>
                    <li>
                        Robust Multi-Agent System
                    </li>
                    <li>
                        AI Alignment
                    </li>
                    <li>
                        Vision-Language-Action Models
                    </li>
                </ul>

            </p>

            I previously work on trustworthy AI for computer vision, including digital world attacks for privacy protection and evaluating naturalness of physical world attacks. Apart from trustworthy AI, I am lucky to work with prominent researchers in various fields, including complex networks, human-computer interaction, robotics, time series forecasting, smart transportation and microelectronics. They have greatly broadened my view and allow me to think in a multidisciplinary way.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading> 

            <p style="font-size:13px"> <strong>[2025.06]</strong> One first-authored paper on MARL attack accepted by Neural Networks</p>

         <p style="font-size:13px"> <strong>[2025.06]</strong> One first-authored paper on robust MARL accepted by IEEE TNNLS</p>

         <p style="font-size:13px"> <strong>[2025.05]</strong> One co-first-authored paper on robust financial trading to be resubmitted to KDD</p>

          <p style="font-size:13px"> <strong>[2024.05]</strong> Five papers submitted to NeurIPS 2025</p>

         <p style="font-size:13px"> <strong>[2024.06]</strong> One first-authored paper submitted to IEEE TPAMI</p>

          <p style="font-size:13px"> <strong>[2024.02]</strong> One co-authored paper on collision avoidance accepted by IEEE RAL</p>

          <p style="font-size:13px"> <strong>[2024.01]</strong> </strong>One first-authored paper on defending Byzantine adversary of MARL accepted by ICLR 2024</strong></p>

          <p style="font-size:13px"> <strong>[2024.01]</strong> One co-authored paper on partial symmetry for MARL accepted by AAAI 2024</p>

          <p style="font-size:13px"> <strong>[2022.11]</strong> </strong>One first-authored paper on naturalness of physical world adversarial attack accepted by CVPR 2023</strong></p>
        
		  <p style="font-size:13px"> <strong>[2022.07]</strong> </strong>One first-authored paper on privacy protection of fingerprints accepted by IEEE TIP.</strong></p>
            
		  <p style="font-size:13px"> <strong>[2022.04]</strong> One co-authored paper on robustness testing of MARL accepted by CVPR 2022 workshop.</p>
                
            </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Selected Publication</heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
    <tr><td width="20%"><img src="./imgs/EPR-MAPPO.png" alt="ICLR2024" width="180" height = "110" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="">
                <papertitle>Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game</papertitle></a>
                <br><strong>Simin Li</strong>, Jun Guo, Jingqiao Xiu, Xin Yu, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu. <br><br>
                Accepted by </em>ICLR</em>, 2024
                <br>
                <a href="https://openreview.net/forum?id=z6KS9D1dxt&referrer=%5Bthe%20profile%20of%20Xianglong%20Liu%5D(%2Fprofile%3Fid%3D~Xianglong_Liu3)">pdf</a>
				 / <a href="https://github.com/DIG-Beihang/EIR-MAPPO"><font color="red">Project page</font></a>
                
                 <p align="justify" style="font-size:13px"> We study robustness of MARL against Byzantine action perturbations by formulating it as a Bayesian game. We provide a rigorious formulation of this problem and an algorithm with strong empirical performance. </p>
            <p></p>
        </td>
    </tr>

    <tr><td width="20%"><img src="./imgs/mir2.png" alt="AAAI2024" width="180" height = "110" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="">
                <papertitle>Mutual Information Regularization is Provably Robust for Multi-Agent Reinforcement Learning</papertitle></a>
                <br><strong>Simin Li</strong>, Ruixiao Xu, Jun Guo, Pu Feng, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu. <br><br>
                Accepted by </em>IEEE TNNLS</em>.
                <br>
                <a href="https://arxiv.org/abs/2310.09833">pdf</a>
                
                <p align="justify" style="font-size:13px"> We proof that minimizing mutual information as a regularization term is minimizing a lower bound of robustness in MARL under all potential threat scenarios. </p>
            <p></p>
        </td>
    </tr>

	<tr><td width="20%"><img src="./imgs/AMI.png" alt="IEEE TCYB" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="">
                 <papertitle>Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence</papertitle></a>
                 <br><strong>Simin Li</strong>,  Jun Guo, Jingqiao Xiu, Pu Feng, Xin Yu, Jiakai Wang, Aishan Liu, Wenjun Wu, Xianglong Liu. <br><br>
                 Accepted by </em>Neural Networks</em>.
                 <br>
                 <a href="https://arxiv.org/abs/2302.03322">pdf</a>
				 / <a href="https://github.com/DIG-Beihang/AMI"><font color="red">Project page</font></a>

                 <p align="justify" style="font-size:13px"> We propose the first adversarial policy attack for c-MARL, which is strong and practical. Our attack provides the first demonstration that adversarial policy is effective against real world robot swarms. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/naturalness.png" alt="CVPR 2023" width="180" height = "160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <papertitle>Towards Benchmarking and Assessing Visual Naturalness of Physical World Adversarial Attacks</papertitle>
                 <br><strong>Simin Li</strong>, Shuning Zhang, Gujun Chen, Dong Wang, Pu Feng, Jiakai Wang, Aishan Liu, Xin Yi, Xianglong Liu.<br>
                 Accepted by <em>CVPR</em>, 2023
				 <br>
                <a href="https://arxiv.org/abs/2305.12863">pdf</a>  /
				<a href="https://github.com/DIG-Beihang/PAN"><font color="red">Project page</font></a>
               
                 <p align="justify" style="font-size:13px"> We take the first step to evaluate the naturalness of physical world adversarial examples by a human oriented approach. We collect the first dataset with human naturalness ratings and human gaze, unveil insights of how contextual and behavioral features will affect attack naturalness, and propose an algorithm to automatically evaluate naturalness by aligning human behavior and algorithm prediction.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/fingerprint.png" alt="TIP" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2208.10688">
                 <papertitle>Hierarchical Perceptual Noise Injection for Social Media Fingerprint Privacy Protection</papertitle></a>
                 <br><strong>Simin Li</strong>, Huangxinxin Xu, Jiakai Wang, Aishan Liu, Fazhi He, Xianglong Liu, Dacheng Tao. <br><br>
                 Accepted by <em>IEEE TIP</em>.
                 <br>
                 <a href="https://arxiv.org/abs/2208.10688">pdf</a>
				 / <a href="https://github.com/nlsde-safety-team/FingerSafe"><font color="red">Project page</font></a>
                
                 <p align="justify" style="font-size:13px"> While billions of people are sharing their daily life images on social media everyday, hackers can easily steal fingerprint from the shared images. We leverage adversarial attack to protect such privacy leakage, such that hackers cannot extract fingerprints even they get the shared images in social media. Our method, FingerSafe, is strong for protection and natural for daily use.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/robusttesting.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Guo_Towards_Comprehensive_Testing_on_the_Robustness_of_Cooperative_Multi-Agent_Reinforcement_CVPRW_2022_paper.html">
                    <papertitle>Towards Comprehensive Testing on the Robustness of Cooperative Multi-agent Reinforcement Learning</papertitle></a>
                    <br>Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, <strong>Simin Li (corresponding author)</strong>.<br>
                    Accepted by <em>CVPR Workshop</em>, 2022
                    <br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Guo_Towards_Comprehensive_Testing_on_the_Robustness_of_Cooperative_Multi-Agent_Reinforcement_CVPRW_2022_paper.html">pdf</a>
                   
                    <p align="justify" style="font-size:13px">We propose a testing framework to evaluate the robustness of multi-agent reinforcement learning (MARL) algoritms from the aspect of observation, action and reward. Our work first point out state-of-the-art MARL algorithms, including QMIX and MAPPO, are non-robust in multiple aspects, and point out the urgent need to test and enhance the robustness of MARL algorithms.</p>
                   <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/symmetry.png" alt="AAAI2024" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                    <p><a href="">
                    <papertitle>Leveraging Partial Symmetry for Multi-Agent Reinforcement Learning</papertitle></a>
                    <br>Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, <strong>Simin Li</strong>, Shuhao Liao, Wenjun Wu. <br><br>
                    Accepted by </em>AAAI</em>, 2024
                    <br>
                    <p align="justify" style="font-size:13px"> Symmetry has been used in MARL as a prior to incorporate domain knowledge in the environment, which enhance sample efficiency and performance. In this paper, we extend symmetry to paritial symmetry that considers uncertainties in environment with non-uniform field, including uneven terrain, wind, etc. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/stsn.png" alt="AAAI2024" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                    <p><a href="">
                    <papertitle>Exploiting Spatio-Temporal Symmetry for Multi-Agent Reinforcement Learning</papertitle></a>
                    <br>Xin Yu, Rongye Shi, Yongkai Tian, Li Wang, Tianhao Peng, <strong>Simin Li</strong>, Pu Feng, Wenjun Wu. <br><br>
                    Submitted to </em>IJCAI</em>, 2024
                    <br>
                    <p align="justify" style="font-size:13px"> Symmetry are everywhere in real world, yet current MARL algorithms are agnostic of such symmetry by design. We extend the idea of symmetry to temporal domain, proposing spatial-temporal symmetry network, which includes adds a stronger induction bias during network training. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/lyapnov.png" alt="AAAI2024" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                    <p><a href="">
                    <papertitle>Lyapunov-Informed Multi-Agent Reinforcement Learning</papertitle></a>
                    <br>Pu Feng, Rongye Shi, Size Wang, Xin Yu, Junkang Liang, Jiakai Wang, <strong>Simin Li</strong>, Wenjun Wu. <br><br>
                    Submitted to </em>IJCAI</em>, 2024
                    <br>
                    <p align="justify" style="font-size:13px"> Many MARL tasks specify certain goal states where special rewards are granted. The optimal policy in such task could be characterized by Lyapnov stability, where the policy asymptotically converge to the goal states from any initial, making the goal states stable equilibria. We formulate such process as a Lyapunov Markov game, and proof it facilitates the training process to find a stable suboptimal policy more easily and then converge to an optimal policy more efficiently. </p>
                <p></p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/SPF-RL.png" alt="AIView" width="180" height = "110" style="border-style: none"></td>
           <td width="80%" valign="top">
                <papertitle>SPF-RL: Multi-robots Collision Avoidance with Soft Potential Field informed reinforcement learning</papertitle>
                <br> Pu Feng, Xin Yu, Wenjun Wu, Yongkai Tian, Junkang Liang, <strong>Simin Li</strong>.<br>
                <em> Submitted to RAL</em>, 2024.
                <br>
                <p align="justify" style="font-size:13px">Motivated by soft potential field theory, we propose an algorithm to avoid collision in robot swarms.  </p>
                   <p></p>
           </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/RL-framework.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                    <papertitle>A Survey on Adversarial Attacks and Defenses for Deep Reinforcement Learning (in Chinese)</papertitle>
                    <br>Aishan Liu, Jun Guo, <strong>Simin Li</strong>, Yisong Xiao, Xianglong Liu, Dacheng Tao.<br>
                    <em>Accepted by Chinese Journal of Computers (计算机学报, top journal in China, CCF-A)</em>, 2023.
                    <br>
                    <p align="justify" style="font-size:13px">We provide a comprehensive survey of attack and defenses for deep reinforcement learning. We first analyze adversarial attacks from the perspectives of state-based, reward-based and action based attacks. Then, we illustrate adversarial defenses from adversarial training, adversarial detection, certified robustness and robust learning. Finally, we investigate interesting topics including adversaries for good and model robustness understanding for DRL, and highlights open issues and future challenges in this field.  </p>
                   <p></p>
            </td>
        </tr>

        <tr><td width="20%"><video width="180" height="110" controls><source src="./imgs/majority.mp4" type="video/mp4"></video></td>
            <td width="80%" valign="top">
                    <papertitle>Simulation Platform and Verification for Adversarial Multi-Agent Reinforcement Learning in Unmanned Aerial Vehicle Swarms (in Chinese)</papertitle>
                    <br>Shuangcheng Liu, <strong>Simin Li (corresponding author)</strong>, Hainan Li, Jingqiao Xiu, Aishan Liu, Xianglong Liu.<br> 
                    <em>Accepted by Journal of Cybersecurity (网络空间安全科学学报, Chinese journal on AI secuity)</em>, 2023.
                    <br>
                    <p align="justify" style="font-size:13px">We provide an AirSim-based unmanned aerial vehicle (UAV) simulator. Based on this simulator, we identify several critical adversarial attacks in multi-UAV combat.  </p>
                   <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/behavioral_dynamics.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                    <papertitle>Behavioral Dynamics and Safety Monitoring Methods for Intelligent Systems (in Chinese)</papertitle>
                    <br><strong>Simin Li</strong>, Jiakai Wang, Aishan Liu, Xianglong Liu.<br>
                    <em>Accepted by Journal of Cybersecurity (网络空间安全科学学报, Chinese journal on AI secuity)</em>, 2023.
                    <br>
                    <p align="justify" style="font-size:13px">We advocate the research on behavioral dynamics, which provides both microscopic and macroscopic understanding on adversarial vulnerability. We argue that combining the search of network science and game theory with AI safety could potentially benefit the understanding on micro information transmission and macro agent-wise intereaction. </p>
                   <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/znaq.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                    <papertitle>Theories and methods for full life cycle intelligent systems security testing</papertitle>
                    <br>Jiakai Wang, Aishan Liu, <strong>Simin Li</strong>, Xianglong Liu, Wenjun Wu.<br>
                    <em>Accepted by Artificial Intelligence Security (智能安全, Chinese journal on AI secuity)</em>, 2023.
                    <br>
                    <p align="justify" style="font-size:13px">We propose our recent insight to test the security of an intelligent system from full life cycles, including vulnerabilities in model training, testing and deployment and their testing techniques. We offer insights on safety standards, safety testing platforms and sketch our method on security evaluation of autonomous driving.</p>
                   <p></p>
            </td>
        </tr>
			
        </tbody>
    </table>
	
     <!--Thesis  -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tbody><tr>
          <td><heading>Highlight Project</heading>
          </tr></tbody>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/Chongming.jpg" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                 <papertitle>重明 (AISafety)</papertitle></a>
                 <br>
                  <br>
                 <a href="https://arxiv.org/pdf/2101.09617.pdf">pdf</a> /
				  <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml"><font color="red"> (News: TechWeb)</font></a> /
				 <a href="https://github.com/DIG-Beihang/AISafety"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=AISafety&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                
                 <p align="justify" style="font-size:13px">重明 is an open-source platform to evaluate model robustness and safety towards noises (e.g., adversarial examples, corruptions, etc.). 
				 The name is taken from the Chinese myth <a href="https://baike.baidu.com/item/%E9%87%8D%E6%98%8E%E9%B8%9F/5482222?fr=aladdin">重明鸟</a>, which has strong power, could fight against beasts and avoid disasters. 
				 We hope our platform could improve the robustness of deep learning systems and help them to avoid safety-related problems. 
				 重明 has been awarded the <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml">首届OpenI启智社区优秀开源项目</a> (First OpenI Excellent Open Source Project).
                 </p>
                <p></p>
            </td>
        </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
            <td width="20%"><img src="./imgs/robustart.png" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
                <td width="80%" valign="top">
                     <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                     <papertitle>RobustART</papertitle></a>
                     <br>
                      <br>
                     <a href="https://arxiv.org/pdf/2109.05211.pdf">pdf</a> /
                      <a href="https://baijiahao.baidu.com/s?id=1711221498985379616&wfr=spider&for=pc"><font color="red"> (News: 机器之心)</font></a> /
                     <a href="http://robust.art/"><font color="red">Project page</font></a>
                     <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=RobustART&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                    
                    
                     <p align="justify" style="font-size:13px">RobustART is the first comprehensive Robustness investigation benchmark on large-scale dataset ImageNet regarding ARchitectural design (49 human-designed off-the-shelf architectures and 1200+ neural architecture searched networks) and Training techniques (10+ general ones e.g., extra training data, etc) towards diverse noises (adversarial, natural, and system noises). 
                     Our benchmark (including open-source toolkit, pre-trained model zoo, datasets, and analyses): 
                     (1) presents an open-source platform for conducting comprehensive evaluation on diverse robustness types; (2) provides a variety of pre-trained models with different training techniques to facilitate robustness evaluation; (3) proposes a new view to better understand the mechanism towards designing robust DNN architectures, backed up by the analysis. 
                     We will continuously contribute to building this ecosystem for the community.
                     </p>
                    <p></p>
                </td>
            </tr>
    
            </tbody>
        </table>-->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
                <td><heading>Academic Services</heading>
                <p><strong>[Workshop@CVPR2023]</strong>I serve as Program Commitee at workshop <a href="https://robustart.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> at CVPR 2023.</p>
		        <p><strong>[Reviewer]</strong>I am a reviewer of CVPR, ECCV, AAAI, Pattern Recognition, etc.</p>
               </td>
               </tr></tbody>
       </table>


     <!--SECTION 7 -->
    <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Main Awards</heading>
            <p> <strong>[2022.06]</strong> &nbsp;&nbsp; Beihang University First Prize Scholarship.</p>
            <p> <strong>[2020.06]</strong> &nbsp;&nbsp; Outstanding Graduates of Beijing Province.</p>
           </td>
           </tr></tbody>
   </table>-->


    <!--SECTION 9 -->
    <!--SECTION 9 -->
    <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=8WoXGe0uVM6u19CuQeHbJkUz_ygkZG6S1688p1DPxb8"></script>
    </p></td>
    </tr>
    </tbody>
    </table>-->
