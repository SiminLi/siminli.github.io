<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Simin Li</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
<!--     <link rel="icon" type="image/jpg" href="https://htqin.github.io/buaa_icon.jpg"> -->
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Simin Li (李思民)</name></p>
                <p align="justify">I am a Ph.D. student (2021.09-) at the State Key Laboratory of Software Development Environment (SKLSDE) and (<a href="https://scse.buaa.edu.cn/">School of Computer Science and Engineering (SCSE)</a>) <a href="https://scse.buaa.edu.cn/"></a> department, <a href="https://www.buaa.edu.cn/">Beihang University</a>, Beijing, China, supervised by Prof. <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>. Before that, I obtained my BSc degree in 2018 from <a href="https://www.buaa.edu.cn/">Beihang University</a> (<i>Summa Cum Laude</i>).
                    <br><br>

                    <strong>Email:</strong> lisiminsimon@buaa.edu.cn
                <br>

                </p><p align="center">
                    <a href="https://github.com/SiminLi"> Github </a> / 
                    <a href="https://scholar.google.com/citations?user=uo8l5ckAAAAJ&hl">Google Scholar</a> / 
<!--                     <a href="https://www.zhihu.com/column/c_1343181023409074176">知乎专栏</a> -->
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">
		    I am interested in <strong>Trustworthy AI</strong> in <strong>Multi-agent reinforcement learning (MARL)</strong>. My research goal is to make reinforcement learning safe and robust, including practical adversarial attack for RL/MARL, adversarial defense and algorithmic robustness testing. 
            </p>
            Now my research mainly includes:
                <ul>
                    <li>
                        Physically realistic adversarial attacks and defenses for MARL
                    </li>
                    <li>
                        Adversarial attacks and defenses driven by multidisciplinary insights and techniques
                    </li>
                    <li>
                        Human computer interaction and human-AI alignment
                    </li>
                    <li>
                        Model robustness evaluation and testing
                    </li>
                </ul>

            </p>
            I previously work on adversarial attacks for computer vision, including digital world attacks for social good and evaluating naturalness of physical world attacks.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading> 
		
		  <p style="font-size:13px"> <strong>[2023.02]</strong> One paper submitted to USENIX Security 23'.</p>

          <p style="font-size:13px"> <strong>[2022.12] One paper accepted by Chinese Journal of Computers (计算机学报, top journal in China, CCF-A).
	
		  <p style="font-size:13px"> <strong>[2022.11]</strong> One paper submitted to CVPR 2022.</p>
        
		  <p style="font-size:13px"> <strong>[2022.07]</strong> One paper submitted to IEEE TIP.</p>
            
		  <p style="font-size:13px"> <strong>[2022.04]</strong> One paper accepted by CVPR 2022 workshop.</p>
              
  
                
            </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Selected Publication</heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
	<tr><td width="20%"><img src="./imgs/AMI.png" alt="USENIX2023" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="">
                 <papertitle>Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence</papertitle></a>
                 <br><strong>Simin Li</strong>,  Jun Guo, Jingqiao Xiu, Pu Feng, Xin Yu, Jiakai Wang, Aishan Liu, Wenjun Wu, Xianglong Liu <br><br>
                 Submitted to </em>USENIX Security</em>, 2023
                 <br>
                 <a href="https://arxiv.org/abs/2302.03322">pdf</a>
				 / <a href="https://anonymous.4open.science/r/AMI/README.md"><font color="red">Project page</font></a>

                 <p align="justify" style="font-size:13px"> We propose the first adversarial policy attack for c-MARL, which is strong and practical. Our attack provides the first demonstration that adversarial policy is effective against real world robot swarms. </p>
                <p></p>
            </td>
        </tr>
        <tr><td width="20%"><img src="./imgs/naturalness.png" alt="CVPR 2023" width="180" height = "160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <papertitle>Towards Benchmarking and Assessing Visual Naturalness of Physical World Adversarial Attacks</papertitle>
                 <br><strong>Simin Li</strong>, Shuning Zhang, Gujun Chen, Dong Wang, Pu Feng, Jiakai Wang, Aishan Liu, Xin Yi, Xianglong Liu.<br>
                 Submitted to <em>CVPR</em>, 2023
				 <br>
                 pdf  /
				<a href="https://anonymous.4open.science/r/naturalEval-DAC1/README.md"><font color="red">Project page</font></a>
               
                 <p align="justify" style="font-size:13px"> We take the first step to evaluate the naturalness of physical world adversarial examples by a human oriented approach. We first contribute Physical Attack Naturalness (PAN) dataset to assist our study by collecting 2688 images with human ratings and human gaze. Based on PAN, we unveil insights of how contextual and behavioral features that affect attack naturalness, and propose an algorithm to automatically evaluate naturalness by aligning human behavior and algorithm prediction.</p>
                <p></p>
            </td>
        </tr>
        <tr><td width="20%"><img src="./imgs/fingerprint.png" alt="TIP" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2208.10688">
                 <papertitle>Universal Adversarial Patch Attack for Automatic Checkout using Perceptual and Attentional Bias</papertitle></a>
                 <br><strong>Jiakai Wang*</strong>, Aishan Liu*,  Xiao Bai, Xianglong Liu <br><br>
                 Submitted to <em>IEEE TIP</em>(IF=10.86)
                 <br>
                 <a href="https://arxiv.org/abs/2208.10688">pdf</a>
				 / <a href="https://github.com/nlsde-safety-team/FingerSafe"><font color="red">Project page</font></a>
                
                 <p align="justify" style="font-size:13px"> While billions of people are sharing their daily life images on social media everyday, hackers can steal fingerprint from easily the shared images. We leverage adversarial attack to protect such privacy leakage, such that our protection is strong in privacy protection and natural for daily use.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/dualattention.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/pdf/2103.01050.pdf">
                    <papertitle>Dual Attention Suppression Attack: Generate Adversarial Camouflage in Physical World</papertitle></a>
                    <br><strong>Jiakai Wang</strong>, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, Xianglong Liu.<br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                    <font color="red"><strong>(Oral)</strong></font>
                    <br>
                    <a href="https://arxiv.org/pdf/2103.01050.pdf">pdf</a> /
                   <font color="red"> News:</font>
                   <a href="https://mp.weixin.qq.com/s/cIcJvmkbvQk-W_qJADkSqw"><font color="red">(机器之心)</font></a>
                   /<a href="https://github.com/nlsde-safety-team/DualAttentionAttack"><font color="red">Project page</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=nlsde-safety-team&repo=DualAttentionAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                   
                    <p align="justify" style="font-size:13px">We propose the Dual Attention Suppression (DAS) attack to generate visually-natural physical adversarial camouflages with strong transferability by suppressing both model and human attention. </p>
                   <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./imgs/ECCV_2.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/pdf/2005.09257.pdf">
                    <papertitle>Bias-based Universal Adversarial Patch Attack for Automatic Check-out</papertitle></a>
                    <br>Aishan Liu*, <strong>Jiakai Wang*</strong>, Xianglong Liu, Bowen Cao, Chongzhi Zhang, Hang Yu.<br>
                    <em>European Conference on Computer Vision (ECCV)</em>, 2020
                    <br>
                    <a href="https://arxiv.org/pdf/2005.09257.pdf">pdf</a> /
                   <font color="red"> News:</font>
                   <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652073635&idx=5&sn=b1acd091996cacb9e74053c4208b793c&chksm=f1201a52c6579344ae75ccb2ee3042ed3eddbb6bd988000c7b1ba8c274f1aceaec7ea1300d1d&mpshare=1&scene=1&srcid=07082kvhcWURQF4VRcIx8uU9&sharer_sharetime=1596855924325&sharer_shareid=da9c9379a79901c18dc93793609d62fa&key=4defdd0e8978fadbc4f7d3467b572eb060d5482035de4befee54b935c6aabbcaafa3ed60343840f82abb27fbcc57798b93e6f215c44f11a37e87141722c58b1167ffeba220d7150c5c8ee9333d06f513&ascene=1&uin=MTQzMTA0NDAw&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=AfGxCG3P9erzoZeJcwLMjbg%3D&pass_ticket=qME0ljezjearOlDwgFgo%2F6ZH0VZ%2B7CLScg%2FNCc5rqMk%3D"><font color="red">(新智元)</font></a>
                   /<a href="https://github.com/nlsde-safety-team/PerceptualAttentionalBiasedAttack"><font color="red">Project page</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=liuaishan&repo=ModelBiasedAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                   
                    <p align="justify" style="font-size:13px">We propose a bias-based framework to generate class-agnostic universal adversarial patches with strong generalization ability, which exploits both the perceptual and semantic bias of models.  </p>
                   <p></p>
            </td>
        </tr>
			
	
        <td width="20%"><img src="./imgs/Eval1.jpg" alt="AIView" width="180" height = "110" style="border-style: none"></td>
           <td width="80%" valign="top">
                <p><a href="http://www.cesi.ac.cn/202007/6566.html">
                <papertitle>人工智能机器学习模型及系统的质量要素和测试方法</papertitle></a>
                <br> <strong>王嘉凯</strong>, 刘艾杉, 刘祥龙<br>
                <em>信息技术与标准化</em>, 2020
                
                <br>
                <a href="http://www.cesi.ac.cn/202007/6566.html">pdf</a> 
               <p></p>
           </td>
        </tr>

	

        </tbody>
    </table>
	
     <!--Thesis  -->
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tbody><tr>
          <td><heading>Highlight Project</heading>
          </tr></tbody>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/Chongming.jpg" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                 <papertitle>重明 (AISafety)</papertitle></a>
                 <br>
                  <br>
                 <a href="https://arxiv.org/pdf/2101.09617.pdf">pdf</a> /
				  <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml"><font color="red"> (News: TechWeb)</font></a> /
				 <a href="https://github.com/DIG-Beihang/AISafety"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=AISafety&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                
                 <p align="justify" style="font-size:13px">重明 is an open-source platform to evaluate model robustness and safety towards noises (e.g., adversarial examples, corruptions, etc.). 
				 The name is taken from the Chinese myth <a href="https://baike.baidu.com/item/%E9%87%8D%E6%98%8E%E9%B8%9F/5482222?fr=aladdin">重明鸟</a>, which has strong power, could fight against beasts and avoid disasters. 
				 We hope our platform could improve the robustness of deep learning systems and help them to avoid safety-related problems. 
				 重明 has been awarded the <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml">首届OpenI启智社区优秀开源项目</a> (First OpenI Excellent Open Source Project).
                 </p>
                <p></p>
            </td>
        </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
            <td width="20%"><img src="./imgs/robustart.png" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
                <td width="80%" valign="top">
                     <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                     <papertitle>RobustART</papertitle></a>
                     <br>
                      <br>
                     <a href="https://arxiv.org/pdf/2109.05211.pdf">pdf</a> /
                      <a href="https://baijiahao.baidu.com/s?id=1711221498985379616&wfr=spider&for=pc"><font color="red"> (News: 机器之心)</font></a> /
                     <a href="http://robust.art/"><font color="red">Project page</font></a>
                     <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=RobustART&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                    
                    
                     <p align="justify" style="font-size:13px">RobustART is the first comprehensive Robustness investigation benchmark on large-scale dataset ImageNet regarding ARchitectural design (49 human-designed off-the-shelf architectures and 1200+ neural architecture searched networks) and Training techniques (10+ general ones e.g., extra training data, etc) towards diverse noises (adversarial, natural, and system noises). 
                     Our benchmark (including open-source toolkit, pre-trained model zoo, datasets, and analyses): 
                     (1) presents an open-source platform for conducting comprehensive evaluation on diverse robustness types; (2) provides a variety of pre-trained models with different training techniques to facilitate robustness evaluation; (3) proposes a new view to better understand the mechanism towards designing robust DNN architectures, backed up by the analysis. 
                     We will continuously contribute to building this ecosystem for the community.
                     </p>
                    <p></p>
                </td>
            </tr>
    
            </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
                <td><heading>Academic Services</heading>
                <p><strong>[Workshop@CVPR2023]</strong>I serve as Program Commitee at workshop <a href="https://robustart.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> at CVPR 2023.</p>
		        <p><strong>[Reviewer]</strong>I am a reviewer of CVPR, ECCV, AAAI, Pattern Recognition, etc.</p>
               </td>
               </tr></tbody>
       </table>


     <!--SECTION 7 -->
    <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Main Awards</heading>
            <p> <strong>[2022.06]</strong> &nbsp;&nbsp; Beihang University First Prize Scholarship.</p>
            <p> <strong>[2020.06]</strong> &nbsp;&nbsp; Outstanding Graduates of Beijing Province.</p>
           </td>
           </tr></tbody>
   </table>-->


    <!--SECTION 9 -->
    <!--SECTION 9 -->
    <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=8WoXGe0uVM6u19CuQeHbJkUz_ygkZG6S1688p1DPxb8"></script>
    </p></td>
    </tr>
    </tbody>
    </table>-->
